{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aecc2467-ea37-48d6-a3ee-ee20cace7def",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src import util as util\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import collections\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "071792e6-6470-460c-a0ac-bde0d87d04ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_spam_dataset_csv(csv_path):\n",
    "\n",
    "    messages = []\n",
    "    labels = []\n",
    "\n",
    "    with open(csv_path, 'r', newline='', encoding='utf8') as csv_file:\n",
    "        reader = csv.reader(csv_file, delimiter=',')\n",
    "\n",
    "        for message, label in reader:\n",
    "            messages.append(message)\n",
    "            labels.append(1 if label == '1' else 0)\n",
    "\n",
    "    return messages, np.array(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f661529d-e36f-40d5-83c7-4a436c30ff00",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_messages, train_labels = util.load_spam_dataset('data/train.tsv')\n",
    "test_messages, test_labels = util.load_spam_dataset('data/test.tsv')\n",
    "train2_messages, train2_labels = load_spam_dataset_csv('data/emails_new_train.csv')\n",
    "val_messages, val_labels = util.load_spam_dataset('data/val.tsv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5d821ec5-336f-4c12-b5f7-4d54d0361c7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\ASUS\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ee439f28-03f6-40e5-966c-744dc2b1a545",
   "metadata": {},
   "outputs": [],
   "source": [
    "# More robust tokenization with error handling\n",
    "def safe_tokenize(text):\n",
    "    try:\n",
    "        return word_tokenize(str(text).lower())\n",
    "    except:\n",
    "        return str(text).lower().split()  # Fallback to simple whitespace splitting\n",
    "\n",
    "tokenized_messages = [safe_tokenize(msg) for msg in train_messages]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9c6c89eb-3c4e-4974-9ba5-466db7d339c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Word2Vec model...\n"
     ]
    }
   ],
   "source": [
    "# Train Word2Vec model with optimized parameters\n",
    "print(\"Training Word2Vec model...\")\n",
    "w2v_model = Word2Vec(\n",
    "    sentences=tokenized_messages,\n",
    "    vector_size=100,\n",
    "    window=5,\n",
    "    min_count=2,\n",
    "    workers=4,\n",
    "    epochs=20,\n",
    "    sg=1,  # Use skip-gram (better for small datasets)\n",
    "    hs=0,  # Use negative sampling\n",
    "    negative=5  # Number of negative samples\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "10aa6c66-4e43-48cf-9cd1-9d59cf432367",
   "metadata": {},
   "outputs": [],
   "source": [
    "def message_to_vector(message, model, vector_size=100):\n",
    "    words = safe_tokenize(message)\n",
    "    word_vectors = []\n",
    "    for word in words:\n",
    "        try:\n",
    "            if word in model.wv:\n",
    "                word_vectors.append(model.wv[word])\n",
    "        except AttributeError:\n",
    "            # Handle case where model isn't properly trained\n",
    "            pass\n",
    "    \n",
    "    if len(word_vectors) > 0:\n",
    "        return np.mean(word_vectors, axis=0)\n",
    "    return np.zeros(vector_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "95154ac4-f2d6-41b4-a414-1f80346c818f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4457"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "81f368bb-363c-4b27-b5c2-b454582aa198",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.06405374,  0.10247273, -0.08413229, -0.01936729, -0.12552986,\n",
       "       -0.36451343,  0.01740931,  0.7463869 , -0.23184416,  0.04113259,\n",
       "       -0.06659532, -0.10470779, -0.09590095,  0.24004175,  0.12144645,\n",
       "        0.04116317,  0.02882743, -0.10803799, -0.28054962, -0.527036  ,\n",
       "       -0.12217733,  0.01034862, -0.2163644 , -0.1567854 , -0.05089254,\n",
       "        0.12351648, -0.09881826,  0.01471897, -0.05641815,  0.16753095,\n",
       "        0.16980436, -0.02056365,  0.14284757,  0.05108713, -0.0401189 ,\n",
       "        0.16103244, -0.2935561 ,  0.04404325,  0.07926708, -0.02141366,\n",
       "        0.20403479, -0.08230109, -0.14010862,  0.293341  ,  0.3314357 ,\n",
       "       -0.21688081, -0.3150358 ,  0.19506751,  0.06765264,  0.05199387,\n",
       "        0.4192219 ,  0.13766335,  0.09758272, -0.00262893, -0.11987112,\n",
       "       -0.06391813,  0.31355217, -0.19861668, -0.13932532, -0.11765104,\n",
       "        0.01603647, -0.04146134,  0.14646047, -0.2999676 ,  0.01217924,\n",
       "        0.3836628 ,  0.29186645,  0.21757127, -0.6328982 ,  0.03086336,\n",
       "       -0.04523571,  0.12726873,  0.16042726, -0.0582362 ,  0.2545184 ,\n",
       "        0.16437276,  0.253546  ,  0.2217859 , -0.17233726, -0.35124546,\n",
       "       -0.22883217,  0.18020707, -0.27859923,  0.4309391 , -0.00249049,\n",
       "        0.07203165,  0.07608946,  0.12666681,  0.3783869 , -0.12646484,\n",
       "        0.19474244,  0.13492383,  0.10176115,  0.30025408,  0.6195323 ,\n",
       "        0.34577343, -0.08275703,  0.10010172,  0.1790308 ,  0.08911305],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "message_to_vector(\"hello\",w2v_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "52c83d15-cf3f-4d50-9409-b488315b3647",
   "metadata": {},
   "outputs": [],
   "source": [
    "Xhh_train = np.array([message_to_vector(msg, w2v_model) for msg in train_messages])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24b469ff-9a16-4427-badb-736f0265b7e4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ae32dcb-dd90-4b9f-9de4-b0a9e253f79e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "419c8550-94c3-4048-aeda-ba3eb59b62a7",
   "metadata": {},
   "source": [
    "already prtrained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ef5ffbe1-23f8-47d5-9813-7bb380fd14f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.style.use('ggplot')\n",
    "from sklearn.decomposition import PCA\n",
    "import gensim.downloader as api\n",
    "from gensim.models import KeyedVectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8f360578-a193-4f77-bb07-0951a14cb93e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pre-trained embeddings (choose one)\n",
    "embeddings = api.load(\"glove-twitter-100\")  # Best for SMS-style text\n",
    "# embeddings = api.load(\"word2vec-google-news-300\")  # Larger vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2104aaf3-fcbb-4e08-abfe-aada7bdd45f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def message_to_vector(messages, embedding_model):\n",
    "    vectors = []\n",
    "    for msg in messages:\n",
    "        words = msg.lower().split()  # Simple tokenizer\n",
    "        word_vecs = [embedding_model[word] for word in words if word in embedding_model]\n",
    "        vectors.append(np.mean(word_vecs, axis=0) if word_vecs else np.zeros(100))\n",
    "    return np.array(vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4f160eea-209a-421b-91ee-f58037536449",
   "metadata": {},
   "outputs": [],
   "source": [
    "Train_matrix3 = message_to_vector(train_messages, embeddings)\n",
    "Test_matrix3 = message_to_vector(test_messages, embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc902d97-5f03-43b3-b9ff-ec5ccf46986b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f07c972-d1da-41a4-9373-498a7fb3697e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9beabc52-dade-4a91-8641-d8a75f865bbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getwords(message):\n",
    "    return message.lower().split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d340c5a0-aa94-4cec-b319-5bd6310af967",
   "metadata": {},
   "outputs": [],
   "source": [
    "def h(theta,x):\n",
    "    return 1 / (1 + np.exp(-np.dot(x, theta)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3a484f66-aa8e-43f3-83cc-a32cc04cf584",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Create_dictionary(messages):\n",
    "    words = [word for message in messages for word in getwords(message)]\n",
    "    words_count = collections.Counter(words)\n",
    "    #word_dic = {(word, count) for word,count in words_count.items() if count>=5}\n",
    "    freq_word = [word for word,count in words_count.items() if count>=10 and word !='subject:']\n",
    "    return {word: count for count, word in enumerate(freq_word)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "081c72e7-64dd-4f13-b916-b9f6995f0506",
   "metadata": {},
   "outputs": [],
   "source": [
    "dicWords = Create_dictionary(train_messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9edb1336-e2be-44a7-a33d-da2efc60e56b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b465ce12-539a-4f19-b97a-94470df6617f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Transform_text(messages, word_dictionary):\n",
    "    m = len(messages)\n",
    "    n = len(word_dictionary)\n",
    "    matrix = np.zeros((m,n), dtype=int)\n",
    "\n",
    "    word_counts = [collections.Counter(getwords(message)) for message in messages]\n",
    "    for i in range(m):\n",
    "        for word, count in word_counts[i].items():\n",
    "            if word in word_dictionary:\n",
    "                matrix[i][word_dictionary[word]] += count\n",
    "\n",
    "    return matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "40729348-154f-47cc-b4be-34b2b21e9d3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_matrix = Transform_text(train_messages,dicWords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "522468ab-1a4f-4af6-a9b8-43c82907a950",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4457, 877)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "35f789b5-5a96-4a73-95af-a14ea89c258e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4457, 100)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Train_matrix3.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "fad314f2-425c-4268-86a3-5d5e0ce665bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_with_bias = np.c_[np.ones(Train_matrix3.shape[0]),Train_matrix3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a9e7d31c-493a-40ce-90aa-e670a02a9686",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegressionWithGD:\n",
    "    def __init__(self,alpha=0.1,iteration=1000):\n",
    "        self.alpha = alpha\n",
    "        self.iteration=iteration\n",
    "        self.theta=None\n",
    "    def h(self,theta, x):\n",
    "        return 1 / (1 + np.exp(-np.dot(x, theta)))\n",
    "    def fit(self,x,y):\n",
    "        lambda_ = 1\n",
    "        n,m = x.shape\n",
    "        X_with_bias = np.c_[np.ones(n),x]\n",
    "        self.theta = np.zeros(m+1)\n",
    "        #Gradient\n",
    "        for i in range(self.iteration):\n",
    "            #With regularization\n",
    "            linearmodel = (np.dot(X_with_bias.T, (self.h(self.theta, X_with_bias)-y))/m)\n",
    "            L1_reg = np.sign(self.theta)*lambda_\n",
    "\n",
    "            L2_reg = lambda_*self.theta\n",
    "            linearmodel += L2_reg\n",
    "            self.theta = self.theta - self.alpha*linearmodel\n",
    "        \n",
    "        \n",
    "    def predict(self,x):\n",
    "        X_with_bias = np.c_[np.ones(x.shape[0]),x]\n",
    "        y_predicted = 1/(1+np.exp(-X_with_bias.dot(self.theta)))\n",
    "        y_result = [1 if i>0.5 else 0 for i in y_predicted]\n",
    "        return y_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6074d896-2ba5-422e-8e70-76a675226237",
   "metadata": {},
   "outputs": [],
   "source": [
    "LR = LogisticRegressionWithGD(alpha=0.01,iteration=1000)\n",
    "LR.fit(Train_matrix3,train_labels)\n",
    "LR_predictions = LR.predict(Test_matrix3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe3047cf-b156-45be-a453-d7fa6f0a421f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b5537c0c-df7b-49e9-a72f-8c434ae33389",
   "metadata": {},
   "outputs": [],
   "source": [
    "LR_accuracy = np.mean(LR_predictions == test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "47edc7fb-e883-47f3-8891-800b26bb2a28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SimpLe Logistic Regression had an accuracy of 0.9086021505376344 on the testing set\n"
     ]
    }
   ],
   "source": [
    "print('SimpLe Logistic Regression had an accuracy of {} on the testing set'.format(LR_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "39874a94-6d63-4794-8b19-36962b495de0",
   "metadata": {},
   "outputs": [],
   "source": [
    "simpletesting=[\n",
    "'buy on site this product now',\n",
    "    'Get new phone free Click this URL',\n",
    "\"Don't forget to Subscribe our page in www.hh.com\",\n",
    "\"Urgent your Account has been compromised. Verify your details now\",\n",
    "    \"Making $5000 a week working from home. No Skills required\",\n",
    "\"Your Package delivery failed. Click here to reschedule\",\n",
    "    \"Act now this offer expires in 24 hours\",\n",
    "    \"Get rich quick with this one simple trick\",\n",
    "    \"Warning: your computer is as risk. Download this antivirus now \"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0d196271-df73-4052-998c-606f93de71d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "testing1_matrix = Transform_text(simpletesting,dicWords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a694bf3a-3899-42fd-bb91-674c6fa5a610",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9, 877)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testing1_matrix.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d6129f0-c145-46ba-8525-63e315f7d65d",
   "metadata": {},
   "source": [
    "use Large dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2ff7d89c-938c-4088-b8fe-337580b73ff4",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary = Create_dictionary(train2_messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "fedcd199-3f1e-46be-a3bf-6422b81d3fda",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 1, ..., 0, 0, 1])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train2_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6aded30f-64b6-4617-a46d-7db193c5fd5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "test2_matrix = Transform_text(test_messages,vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "62cdde29-e374-4214-aca9-ce9835e76a3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train2_matrix = Transform_text(train2_messages,vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0f77d308-4a3e-4bfc-af09-1e6d90813e6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "Train_matrix32 = message_to_vector(train2_messages, embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "fe9cd66a-d19d-4e50-b195-018d8007fafe",
   "metadata": {},
   "outputs": [],
   "source": [
    "LR2 = LogisticRegressionWithGD(alpha=0.01,iteration=1000)\n",
    "LR2.fit(Train_matrix32,train2_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e7710be3-66e9-4c90-a769-a8a5cfb5beb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "LR_predictions2 = LR2.predict(Test_matrix3)\n",
    "LR_accuracy2 = np.mean(LR_predictions2 == test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "5020fddf-c177-4d61-8217-2a60e9c201e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SimpLe Logistic Regression with large dataset had an accuracy of 0.8727598566308243 on the testing set\n"
     ]
    }
   ],
   "source": [
    "print('SimpLe Logistic Regression with large dataset had an accuracy of {} on the testing set'.format(LR_accuracy2))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
