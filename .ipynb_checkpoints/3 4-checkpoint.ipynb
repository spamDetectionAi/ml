{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c452f3d-cdb4-440b-bcbd-bec7cb31ccb1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1b08774f-d892-4a3a-8c75-4dd3d8789038",
   "metadata": {},
   "outputs": [
    {
     "ename": "SystemError",
     "evalue": "GPU device not found",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mSystemError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 7\u001b[39m\n\u001b[32m      5\u001b[39m device_name = tf.test.gpu_device_name()\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m device_name != \u001b[33m'\u001b[39m\u001b[33m/device:GPU:0\u001b[39m\u001b[33m'\u001b[39m:\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mSystemError\u001b[39;00m(\u001b[33m'\u001b[39m\u001b[33mGPU device not found\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m      8\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m'\u001b[39m\u001b[33mFound GPU at: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[33m'\u001b[39m.format(device_name))\n",
      "\u001b[31mSystemError\u001b[39m: GPU device not found"
     ]
    }
   ],
   "source": [
    "# Check if GPU is found\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "device_name = tf.test.gpu_device_name()\n",
    "if device_name != '/device:GPU:0':\n",
    "  raise SystemError('GPU device not found')\n",
    "print('Found GPU at: {}'.format(device_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18faeef5-da55-459f-a7a5-ec80735393c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify the GPU\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "n_gpu = torch.cuda.device_count()\n",
    "torch.cuda.get_device_name(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5471dbb9-3841-44d0-9c92-6f16975a36e0",
   "metadata": {},
   "source": [
    "Tesla P100-PCIE-16GB'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3f3aa03-7ce4-412d-931c-1d91fa085021",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sentence and label lists\n",
    "sentences = df_new[\"Text\"].values\n",
    "\n",
    "# We need to add special tokens at the beginning and end of each sentence for BERT to work properly\n",
    "sentences = [\"[CLS] \" + sentence + \" [SEP]\" for sentence in sentences]\n",
    "labels = df_new[\"Impact\"].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c2b58cd-e1ef-48b5-9f2f-8e1eaebbc213",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import BERT tokenizer\n",
    "from pytorch_pretrained_bert import BertTokenizer, BertConfi\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n",
    "\n",
    "tokenized_texts = [tokenizer.tokenize(sent) for sent in sentences]\n",
    "print (\"Tokenize the first sentence:\")\n",
    "print (tokenized_texts[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6a1b5f2-38f1-4b4e-ac27-49bb458b555c",
   "metadata": {},
   "source": [
    "Tokenize the first sentence:\n",
    "['[CLS]', 'f', '##wc', '##r', '/', 'proxy', 'change', 'request', '[SEP]']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bc05686-def8-4ed2-87f5-d4dbf3e9d1a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the maximum sequence length. The longest sequence in our training set is 47, but we'll leave room on the end anyway. \n",
    "# In the original paper, the authors used a length of 512.\n",
    "MAX_LEN = 128\n",
    "\n",
    "# Pad our input tokens\n",
    "input_ids = pad_sequences([tokenizer.convert_tokens_to_ids(txt) for txt in tokenized_texts],\n",
    "                          maxlen=MAX_LEN, dtype=\"long\", truncating=\"post\", padding=\"post\")\n",
    "\n",
    "# Use the BERT tokenizer to convert the tokens to their index numbers in the BERT vocabulary\n",
    "input_ids = [tokenizer.convert_tokens_to_ids(x) for x in tokenized_texts]\n",
    "\n",
    "input_ids = pad_sequences(input_ids, maxlen=MAX_LEN, dtype=\"long\", truncating=\"post\", padding=\"post\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b7b6a10-bcfb-4759-8f45-d077e518a606",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create attention masks\n",
    "attention_masks = []\n",
    "\n",
    "# Create a mask of 1s for each token followed by 0s for padding\n",
    "for seq in input_ids:\n",
    "  seq_mask = [float(i>0) for i in seq]\n",
    "  attention_masks.append(seq_mask)\n",
    "  \n",
    "# Use train_test_split to split our data into train and validation sets for training\n",
    "\n",
    "train_inputs, validation_inputs, train_labels, validation_labels = train_test_split(input_ids, labels, \n",
    "                                                            random_state=2018, test_size=0.1)\n",
    "train_masks, validation_masks, _, _ = train_test_split(attention_masks, input_ids,\n",
    "                                             random_state=2018, test_size=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12827fc4-c0f7-4838-b037-aba8a6a84036",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert all of our data into torch tensors, the required datatype for our model\n",
    "\n",
    "train_inputs = torch.tensor(train_inputs)\n",
    "validation_inputs = torch.tensor(validation_inputs)\n",
    "train_labels = torch.tensor(train_labels)\n",
    "validation_labels = torch.tensor(validation_labels)\n",
    "train_masks = torch.tensor(train_masks)\n",
    "validation_masks = torch.tensor(validation_masks)\n",
    "\n",
    "# Select a batch size for training. For fine-tuning BERT on a specific task, the authors recommend a batch size of 16 or 32\n",
    "batch_size = 32\n",
    "\n",
    "# Create an iterator of our data with torch DataLoader. This helps save on memory during training because, unlike a for loop, \n",
    "# with an iterator the entire dataset does not need to be loaded into memory\n",
    "\n",
    "train_data = TensorDataset(train_inputs, train_masks, train_labels)\n",
    "train_sampler = RandomSampler(train_data)\n",
    "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
    "\n",
    "validation_data = TensorDataset(validation_inputs, validation_masks, validation_labels)\n",
    "validation_sampler = SequentialSampler(validation_data)\n",
    "validation_dataloader = DataLoader(validation_data, sampler=validation_sampler, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f101d790-baf1-4d4c-93ea-7b560b1a1ea4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load BertForSequenceClassification, the pretrained BERT model with a single linear classification layer on top. \n",
    "\n",
    "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2, output_hidden_states=True)\n",
    "model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f14dacba-4b17-4d38-bf5a-8f2e9f5fb440",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f20b74c0-5d36-4fe5-a1b6-27c5afe8861d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src import util as util\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import collections\n",
    "from pathlib import Path\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a388c305-5ae8-4bd0-a5f0-4ece1209540c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_spam_dataset_csv(csv_path):\n",
    "\n",
    "    messages = []\n",
    "    labels = []\n",
    "\n",
    "    with open(csv_path, 'r', newline='', encoding='utf8') as csv_file:\n",
    "        reader = csv.reader(csv_file, delimiter=',')\n",
    "\n",
    "        for message, label in reader:\n",
    "            messages.append(message)\n",
    "            labels.append(1 if label == '1' else 0)\n",
    "\n",
    "    return messages, np.array(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b6895953-d3c4-4bad-a859-2966e2040aad",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_messages, train_labels = util.load_spam_dataset('data/train.tsv')\n",
    "test_messages, test_labels = util.load_spam_dataset('data/test.tsv')\n",
    "train2_messages, train2_labels = load_spam_dataset_csv('data/emails_new_train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "97ae9ef7-6926-468a-bd28-9b370792fb20",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ASUS\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\ASUS\\AppData\\Roaming\\Python\\Python311\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "import numpy as np\n",
    "\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')  # Lightweight BERT variant\n",
    "def get_embeddings(email_bodies):\n",
    "    return model.encode(email_bodies, convert_to_numpy=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "bef2a89a-f11e-449a-8e9a-a018049bfac4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertModel\n",
    "import torch\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Initialize BERT\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertModel.from_pretrained('bert-base-uncased')\n",
    "model.eval()  # Disable dropout for consistent embeddings\n",
    "\n",
    "def get_embeddings_Pooling(email_bodies, batch_size=32):\n",
    "    \"\"\"Equivalent to sentence-transformers' encode() but with mean pooling\"\"\"\n",
    "    embeddings = []\n",
    "    \n",
    "    for i in tqdm(range(0, len(email_bodies), batch_size), \n",
    "                 desc=\"Generating embeddings\"):\n",
    "        batch = email_bodies[i:i + batch_size]\n",
    "        \n",
    "        # Tokenize with BERT's conventions\n",
    "        inputs = tokenizer(\n",
    "            batch,\n",
    "            return_tensors=\"pt\",\n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "            max_length=128,\n",
    "            add_special_tokens=True  # Adds [CLS] and [SEP]\n",
    "        )\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "        \n",
    "        # Mean pooling (exclude special tokens)\n",
    "        last_hidden = outputs.last_hidden_state\n",
    "        attention_mask = inputs['attention_mask']\n",
    "        \n",
    "        # Expand mask to match embedding dim\n",
    "        mask = attention_mask.unsqueeze(-1).expand(last_hidden.size()).float()\n",
    "        \n",
    "        # Sum embeddings (ignoring padding)\n",
    "        sum_embeddings = torch.sum(last_hidden * mask, dim=1)\n",
    "        \n",
    "        # Count non-padding tokens\n",
    "        sum_mask = torch.sum(mask, dim=1)\n",
    "        sum_mask = torch.clamp(sum_mask, min=1e-9)  # Avoid division by zero\n",
    "        \n",
    "        # Mean pooling\n",
    "        batch_embeddings = (sum_embeddings / sum_mask).numpy()\n",
    "        embeddings.extend(batch_embeddings)\n",
    "    \n",
    "    return np.array(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7a5d17f5-7c90-444c-b0db-0b1454a4b065",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating embeddings: 100%|█████████████████████████████████████████████████████████| 140/140 [14:32<00:00,  6.23s/it]\n"
     ]
    }
   ],
   "source": [
    "matrix1 = get_embeddings_Pooling(train_messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "02bc60aa-fa22-4080-bf33-9b9204d50af5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating embeddings: 100%|███████████████████████████████████████████████████████████| 18/18 [00:53<00:00,  2.95s/it]\n"
     ]
    }
   ],
   "source": [
    "matrix2 = get_embeddings_Pooling(test_messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bffd0e64-1c8a-435b-972b-723d49b636e1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96adb7c6-e6c3-479d-b8e7-4b34178e1bb6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f68f1dea-db5f-4b18-899d-42229352adfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix_training = get_embeddings(train_messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ad1e278c-dcb1-413b-a476-db4d0beee1c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix_test = get_embeddings(test_messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f6d58b5e-c187-4a0f-83a7-c23f6ddac2ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4457, 384)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matrix_training.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "422ac741-3ca5-45db-91b1-0f9a6933a809",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "# 2. Train SVM\n",
    "svm = SVC(kernel='linear', C=1.0, class_weight='balanced', random_state=42)\n",
    "svm.fit(matrix_training, train_labels)\n",
    "\n",
    "# 3. Predict\n",
    "predictions = svm.predict(matrix_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8426b2bf-4108-4dfa-b24e-f00838c7e698",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9910394265232975\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "print(\"Accuracy:\", accuracy_score(test_labels, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5a7b1fed-1d1d-41d8-b5fe-b37160ad37d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_messages_simple = [\n",
    "    # Non-Spam (Ham) - 15 examples\n",
    "    \"Hi John, just checking in to see if you're still on for lunch tomorrow at 12:30 PM.\",\n",
    "    \"The quarterly financial report has been uploaded to the shared drive for your review.\",\n",
    "    \"Team meeting reminder: Wednesday at 3 PM in Conference Room A. Agenda attached.\",\n",
    "    \"Your Amazon order #12345 has shipped and will arrive on Friday.\",\n",
    "    \"Thanks for your application! We'll review your resume and get back to you next week.\",\n",
    "    \"The software update has been completed successfully on all servers.\",\n",
    "    \"Mom: Don't forget we're having family dinner this Sunday at 6 PM.\",\n",
    "    \"Your monthly bank statement is now available in your online banking portal.\",\n",
    "    \"The project deadline has been extended to March 15th per client request.\",\n",
    "    \"Password reset confirmation: Your password was changed successfully.\",\n",
    "    \"Doctor's appointment reminder: You have a checkup scheduled for May 3rd at 10 AM.\",\n",
    "    \"Your subscription to Tech Magazine has been renewed automatically.\",\n",
    "    \"The attached document contains the meeting minutes from yesterday's call.\",\n",
    "    \"Your flight LAX to JFK is confirmed for departure at 8:45 AM tomorrow.\",\n",
    "    \"HR Notification: Please complete your benefits enrollment by Friday.\",\n",
    "\n",
    "    # Spam - 5 examples\n",
    "    \"URGENT: Your account will be suspended unless you verify your details now!\",\n",
    "    \"CONGRATULATIONS! You've won a free iPhone - click here to claim your prize!\",\n",
    "    \"Make $10,000 a week from home with this simple trick! No experience needed!\",\n",
    "    \"Your package couldn't be delivered - click this link to reschedule immediately!\",\n",
    "    \"Limited time offer! Act now to get 90% off - this deal expires in 1 hour!\"\n",
    "]\n",
    "\n",
    "matrix_15_5 = get_embeddings(test_messages_simple)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "12346098-1c46-4b7c-b1b3-7d95e124caac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "predictions_finale = svm.predict(matrix_15_5)\n",
    "predictions_finale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0daa3e5c-51a9-4951-9f6b-05a801d0815c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7a097ba-eea7-4bf3-bd30-a9dbde70bd41",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9400c92b-db54-49d8-8a72-38682d2f66f4",
   "metadata": {},
   "source": [
    "Large 10000 dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "268e4f25-b017-4c8c-ab35-767356950f1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "matrix_100 = get_embeddings(train2_messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "372f2a7e-48a0-4ae6-8617-14b698361e35",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import classification_report, f1_score\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "# 2. Optimized SVM Pipeline\n",
    "svm_pipeline = Pipeline([\n",
    "    ('scaler', StandardScaler()),  # Critical for SVM\n",
    "    ('svm', SVC(\n",
    "        kernel='rbf',              # Better for complex patterns\n",
    "        class_weight='balanced',\n",
    "        probability=True,          # Enable predict_proba\n",
    "        cache_size=1000,           # For large datasets\n",
    "        random_state=42\n",
    "    ))\n",
    "])\n",
    "\n",
    "# 3. Hyperparameter Tuning (Reduced search space for efficiency)\n",
    "param_dist = {\n",
    "    'svm__C': [0.1, 1, 10, 100],\n",
    "    'svm__gamma': ['scale', 'auto', 0.001, 0.01]\n",
    "}\n",
    "\n",
    "search = RandomizedSearchCV(\n",
    "    svm_pipeline,\n",
    "    param_dist,\n",
    "    n_iter=10,                    # Reduced for faster tuning\n",
    "    scoring='f1_weighted',\n",
    "    cv=3,\n",
    "    n_jobs=-1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d62b8109-d228-490c-bbba-c3fb3bee760b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.46953405017921146\n"
     ]
    }
   ],
   "source": [
    "search.fit(matrix_100,train2_labels)\n",
    "simple_svm_3 = search.predict(matrix_test)\n",
    "from sklearn.metrics import accuracy_score\n",
    "print(\"Accuracy:\", accuracy_score(test_labels, simple_svm_3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a42928d3-0356-4f9e-bee5-87ce62aa1239",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.996415770609319\n"
     ]
    }
   ],
   "source": [
    "search.fit(matrix1,train_labels)\n",
    "simple_svm_4 = search.predict(matrix2)\n",
    "from sklearn.metrics import accuracy_score\n",
    "print(\"Accuracy:\", accuracy_score(test_labels, simple_svm_4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "2115ade8-1342-4729-a655-ad5c6b108e8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating embeddings: 100%|█████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  1.10it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "matrix4 = get_embeddings_Pooling(test_messages_simple)\n",
    "simple_svm_5 = search.predict(matrix4)\n",
    "simple_svm_5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5da0e037-994e-4153-8ab4-3af38b4f69cc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
