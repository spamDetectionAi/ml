{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "aecc2467-ea37-48d6-a3ee-ee20cace7def",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src import util as util\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import collections\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "071792e6-6470-460c-a0ac-bde0d87d04ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_spam_dataset_csv(csv_path):\n",
    "\n",
    "    messages = []\n",
    "    labels = []\n",
    "\n",
    "    with open(csv_path, 'r', newline='', encoding='utf8') as csv_file:\n",
    "        reader = csv.reader(csv_file, delimiter=',')\n",
    "\n",
    "        for message, label in reader:\n",
    "            messages.append(message)\n",
    "            labels.append(1 if label == '1' else 0)\n",
    "\n",
    "    return messages, np.array(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "f661529d-e36f-40d5-83c7-4a436c30ff00",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_messages, train_labels = util.load_spam_dataset('data/train.tsv')\n",
    "test_messages, test_labels = util.load_spam_dataset('data/test.tsv')\n",
    "train2_messages, train2_labels = load_spam_dataset_csv('data/emails_new_train.csv')\n",
    "val_messages, val_labels = util.load_spam_dataset('data/val.tsv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68644109-25e1-4d30-9067-8cb20e1b10ad",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ad35d48-f1ca-43a2-8c5f-1860ecd8c73a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ff2ea0b-07c0-407b-8c35-4f1597489ac3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24b469ff-9a16-4427-badb-736f0265b7e4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f360578-a193-4f77-bb07-0951a14cb93e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "9beabc52-dade-4a91-8641-d8a75f865bbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getwords(message):\n",
    "    return message.lower().split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "d340c5a0-aa94-4cec-b319-5bd6310af967",
   "metadata": {},
   "outputs": [],
   "source": [
    "def h(theta,x):\n",
    "    return 1 / (1 + np.exp(-np.dot(x, theta)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "3a484f66-aa8e-43f3-83cc-a32cc04cf584",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Create_dictionary(messages):\n",
    "    words = [word for message in messages for word in getwords(message)]\n",
    "    words_count = collections.Counter(words)\n",
    "    #word_dic = {(word, count) for word,count in words_count.items() if count>=5}\n",
    "    freq_word = [word for word,count in words_count.items() if count>=10 and word !='subject:']\n",
    "    return {word: count for count, word in enumerate(freq_word)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "081c72e7-64dd-4f13-b916-b9f6995f0506",
   "metadata": {},
   "outputs": [],
   "source": [
    "dicWords = Create_dictionary(train_messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3d275c0-7610-4e7d-a070-75fe366e953b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "cafbbad5-a00e-4814-876e-4cec7aef84e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package reuters to\n",
      "[nltk_data]     C:\\Users\\ASUS\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package reuters is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# All Import Statements Defined Here\n",
    "# Note: Do not add to this list.\n",
    "# ----------------\n",
    "\n",
    "import sys\n",
    "assert sys.version_info[0]==3\n",
    "assert sys.version_info[1] >= 5\n",
    "\n",
    "from platform import python_version\n",
    "assert int(python_version().split(\".\")[1]) >= 5, \"Please upgrade your Python version following the instructions in \\\n",
    "    the README.txt file found in the same directory as this notebook. Your Python version is \" + python_version()\n",
    "\n",
    "from gensim.models import KeyedVectors\n",
    "from gensim.test.utils import datapath\n",
    "import pprint\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['figure.figsize'] = [10, 5]\n",
    "\n",
    "import nltk\n",
    "nltk.download('reuters') #to specify download location, optionally add the argument: download_dir='/specify/desired/path/'\n",
    "from nltk.corpus import reuters\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "import scipy as sp\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "START_TOKEN = '<START>'\n",
    "END_TOKEN = '<END>'\n",
    "\n",
    "np.random.seed(0)\n",
    "random.seed(0)\n",
    "# ----------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eca67d62-9f6c-4051-ac60-c7854edf50f5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "513137c2-cda6-4e4a-a750-cde43477beaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def distinct_words(corpus):\n",
    "    \"\"\" Determine a list of distinct words for the corpus.\n",
    "        Params:\n",
    "            corpus (list of list of strings): corpus of documents\n",
    "        Return:\n",
    "            corpus_words (list of strings): sorted list of distinct words across the corpus\n",
    "            n_corpus_words (integer): number of distinct words across the corpus\n",
    "    \"\"\"\n",
    "    corpus_words = []\n",
    "    n_corpus_words = -1\n",
    "    \n",
    "    ### SOLUTION BEGIN\n",
    "#     for docs in corpus:\n",
    "#         for word in docs:\n",
    "#             corpus_words.append(word)\n",
    "    corpus_words = sorted(list(set([word for docs in corpus for word in docs])))\n",
    "    n_corpus_words = len(corpus_words)\n",
    "    ### SOLUTION END\n",
    "\n",
    "    return corpus_words, n_corpus_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0ce13525-adaf-481c-b1d7-1d64942a326c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_co_occurrence_matrix(corpus, window_size=4):\n",
    "    \"\"\" Compute co-occurrence matrix for the given corpus and window_size (default of 4).\n",
    "    \n",
    "        Note: Each word in a document should be at the center of a window. Words near edges will have a smaller\n",
    "              number of co-occurring words.\n",
    "              \n",
    "              For example, if we take the document \"<START> All that glitters is not gold <END>\" with window size of 4,\n",
    "              \"All\" will co-occur with \"<START>\", \"that\", \"glitters\", \"is\", and \"not\".\n",
    "    \n",
    "        Params:\n",
    "            corpus (list of list of strings): corpus of documents\n",
    "            window_size (int): size of context window\n",
    "        Return:\n",
    "            M (a symmetric numpy matrix of shape (number of unique words in the corpus , number of unique words in the corpus)): \n",
    "                Co-occurence matrix of word counts.\n",
    "                The ordering of the words in the rows/columns should be the same as the ordering of the words given by the distinct_words function.\n",
    "            word2ind (dict): dictionary that maps word to index (i.e. row/column number) for matrix M.\n",
    "    \"\"\"\n",
    "    words, n_words = distinct_words(corpus)\n",
    "    M = None\n",
    "    word2ind = {}\n",
    "    \n",
    "    ### SOLUTION BEGIN\n",
    "    M = np.zeros(shape=(n_words, n_words))\n",
    "    word2ind = {w:i for i, w in enumerate(words)}\n",
    "    for doc in corpus:\n",
    "        doc_size = len(doc)\n",
    "        for i, word in enumerate(doc):\n",
    "            center = word2ind[word]\n",
    "            start = 0 if i <= window_size else i - window_size\n",
    "            end = doc_size if doc_size - i <= window_size + 1 else i + window_size + 1\n",
    "            context = [word for cont in (doc[start:i], doc[i+1:end]) for word in cont]\n",
    "            for w in context:\n",
    "                out = word2ind[w]\n",
    "                M[center][out] += 1\n",
    "    ### SOLUTION END\n",
    "\n",
    "    return M, word2ind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5d51dcec-667a-4f49-ba1b-58b104ec1dea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduce_to_k_dim(M, k=2):\n",
    "    \"\"\" Reduce a co-occurence count matrix of dimensionality (num_corpus_words, num_corpus_words)\n",
    "        to a matrix of dimensionality (num_corpus_words, k) using the following SVD function from Scikit-Learn:\n",
    "            - http://scikit-learn.org/stable/modules/generated/sklearn.decomposition.TruncatedSVD.html\n",
    "    \n",
    "        Params:\n",
    "            M (numpy matrix of shape (number of unique words in the corpus , number of unique words in the corpus)): co-occurence matrix of word counts\n",
    "            k (int): embedding size of each word after dimension reduction\n",
    "        Return:\n",
    "            M_reduced (numpy matrix of shape (number of corpus words, k)): matrix of k-dimensinal word embeddings.\n",
    "                    In terms of the SVD from math class, this actually returns U * S\n",
    "    \"\"\"    \n",
    "    n_iters = 10     # Use this parameter in your call to `TruncatedSVD`\n",
    "    M_reduced = None\n",
    "    print(\"Running Truncated SVD over %i words...\" % (M.shape[0]))\n",
    "    \n",
    "    ### SOLUTION BEGIN\n",
    "    svd = TruncatedSVD(n_components=k, n_iter=n_iters)\n",
    "    M_reduced = svd.fit_transform(M)\n",
    "    ### SOLUTION END\n",
    "\n",
    "    print(\"Done.\")\n",
    "    return M_reduced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4b943303-8860-4f60-8732-25bca31554b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_embedding_model():\n",
    "    \"\"\" Load GloVe Vectors\n",
    "        Return:\n",
    "            wv_from_bin: All 400000 embeddings, each lengh 200\n",
    "    \"\"\"\n",
    "    import gensim.downloader as api\n",
    "    wv_from_bin = api.load(\"glove-twitter-100\")\n",
    "    print(\"Loaded vocab size %i\" % len(list(wv_from_bin.index_to_key)))\n",
    "    return wv_from_bin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "872cfa2e-b3f6-4a60-aaa8-90d93c7c77be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded vocab size 1193514\n"
     ]
    }
   ],
   "source": [
    "wv_from_bin = load_embedding_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d43be986-26cb-426c-b577-ad651dd01802",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1193514"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(wv_from_bin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e7e624ea-0082-4c90-9662-3d45478a1ed1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_matrix_of_vectors(wv_from_bin,bodies):\n",
    "    \"\"\" Put the GloVe vectors into a matrix M.\n",
    "        Param:\n",
    "            wv_from_bin: KeyedVectors object; the 400000 GloVe vectors loaded from file\n",
    "        Return:\n",
    "            M: numpy matrix shape (num words, 200) containing the vectors\n",
    "            word2ind: dictionary mapping each word to its row number in M\n",
    "    \"\"\"\n",
    "    all_body_embeddings = []\n",
    "    word_count=[]\n",
    "    for body in bodies:\n",
    "        body_embeddings = []\n",
    "        words = body.split()  # Split body into words\n",
    "        \n",
    "        for word in words:\n",
    "            try:\n",
    "                # Get word vector (e.g., length 5)\n",
    "                word_vec = wv_from_bin.get_vector(word)\n",
    "                body_embeddings.append(word_vec.tolist())  # Convert numpy array to list\n",
    "            except KeyError:\n",
    "                # Skip words not in vocabulary\n",
    "                continue\n",
    "        \n",
    "        if body_embeddings:  # Only add if we found embeddings\n",
    "            all_body_embeddings.append(body_embeddings)\n",
    "            word_count = len(body_embeddings)\n",
    "         # Pad sequences to make them uniform (for true 3D array)\n",
    "    max_words = max(word_count) if word_count else 0\n",
    "    embedding_dim = wv_from_bin.vector_size\n",
    "    \n",
    "    # Create padded 3D array\n",
    "    M = np.zeros((len(all_body_embeddings), max_words, embedding_dim))\n",
    "    for i, body_emb in enumerate(all_body_embeddings):\n",
    "        M[i, :word_count[i]] = body_emb  # Fill available words\n",
    "    \n",
    "            \n",
    "    return M,word_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "411611b9-236b-4fb7-8f31-6c0d5f7fc858",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "efc2840a-1a1d-49a7-b91c-a3db6d65a43a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_matrix_of_vectors(wv_from_bin,bodies):\n",
    "    \"\"\" Put the GloVe vectors into a matrix M.\n",
    "        Param:\n",
    "            wv_from_bin: KeyedVectors object; the 400000 GloVe vectors loaded from file\n",
    "        Return:\n",
    "            M: numpy matrix shape (num words, 200) containing the vectors\n",
    "            word2ind: dictionary mapping each word to its row number in M\n",
    "    \"\"\"\n",
    "    all_body_embeddings = []\n",
    "    word_count=[]\n",
    "    for body in bodies:\n",
    "        body_embeddings = []\n",
    "        words = body.split()  # Split body into words\n",
    "        \n",
    "        for word in words:\n",
    "            try:\n",
    "                # Get word vector (e.g., length 5)\n",
    "                word_vec = wv_from_bin.get_vector(word)\n",
    "                body_embeddings.append(word_vec.tolist())  # Convert numpy array to list\n",
    "            except KeyError:\n",
    "                body_embeddings.append(np.zeros(100))\n",
    "                continue\n",
    "        \n",
    "        if body_embeddings:  # Only add if we found embeddings\n",
    "            all_body_embeddings.append(body_embeddings)\n",
    "            word_count = len(body_embeddings)\n",
    "         # Pad sequences to make them uniform (for true 3D array)\n",
    "    max_words = max(word_count) if word_count else 0\n",
    "    embedding_dim = wv_from_bin.vector_size\n",
    "    \n",
    "    # Create padded 3D array\n",
    "    M = np.zeros((len(all_body_embeddings), max_words, embedding_dim))\n",
    "    for i, body_emb in enumerate(all_body_embeddings):\n",
    "        M[i, :word_count[i]] = body_emb  # Fill available words\n",
    "    \n",
    "            \n",
    "    return M,word_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b621c3c2-2070-40a4-82c4-d162ad1b8454",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "# 1. Get the matrix and counts\n",
    "M, word_counts = get_matrix_of_vectors(wv_from_bin, train_messages)\n",
    "\n",
    "# 2. Compute average embeddings\n",
    "embedding_dim = M.shape[2]\n",
    "avg_embeddings = np.zeros((len(train_messages), embedding_dim))\n",
    "\n",
    "for i, (total_words, valid_words) in enumerate(word_counts):\n",
    "    if valid_words > 0:\n",
    "        avg_embeddings[i] = np.mean(M[i][:valid_words], axis=0)\n",
    "\n",
    "# 3. Now apply PCA\n",
    "pca = PCA(n_components=20)\n",
    "reduced_embeddings = pca.fit_transform(avg_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "988a97e1-dced-4d75-8e48-8bfc4ed7c6b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4457, 171, 100)"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "M.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "afaf1549-ec6e-4af3-b5cc-47ce1ca57575",
   "metadata": {},
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 72.0 GiB for an array with shape (10907, 8862, 100) and data type float64",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mMemoryError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[99]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m#for large data set\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;66;03m# 1. Get the matrix and counts\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m M_2, word_counts_2 = \u001b[43mget_matrix_of_vectors\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwv_from_bin\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain2_messages\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      5\u001b[39m \u001b[38;5;66;03m# 2. Compute average embeddings\u001b[39;00m\n\u001b[32m      6\u001b[39m embedding_dim_2 = M.shape[\u001b[32m2\u001b[39m]\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[74]\u001b[39m\u001b[32m, line 20\u001b[39m, in \u001b[36mget_matrix_of_vectors\u001b[39m\u001b[34m(wv_from_bin, bodies)\u001b[39m\n\u001b[32m     17\u001b[39m max_words = \u001b[38;5;28mmax\u001b[39m(\u001b[38;5;28mlen\u001b[39m(body.split()) \u001b[38;5;28;01mfor\u001b[39;00m body \u001b[38;5;129;01min\u001b[39;00m bodies) \u001b[38;5;28;01mif\u001b[39;00m bodies \u001b[38;5;28;01melse\u001b[39;00m \u001b[32m0\u001b[39m\n\u001b[32m     19\u001b[39m \u001b[38;5;66;03m# Initialize the 3D array with zeros (all bodies, max words, embedding dim)\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m20\u001b[39m M = np.zeros((\u001b[38;5;28mlen\u001b[39m(bodies), max_words, embedding_dim))\n\u001b[32m     21\u001b[39m word_counts = []\n\u001b[32m     23\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i, body \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(bodies):\n",
      "\u001b[31mMemoryError\u001b[39m: Unable to allocate 72.0 GiB for an array with shape (10907, 8862, 100) and data type float64"
     ]
    }
   ],
   "source": [
    "#for large data set\n",
    "# 1. Get the matrix and counts\n",
    "M_2, word_counts_2 = get_matrix_of_vectors(wv_from_bin, train2_messages)\n",
    "\n",
    "# 2. Compute average embeddings\n",
    "embedding_dim_2 = M.shape[2]\n",
    "avg_embeddings_2 = np.zeros((len(train2_messages), embedding_dim_2))\n",
    "\n",
    "for i, (total_words, valid_words) in enumerate(word_counts):\n",
    "    if valid_words > 0:\n",
    "        avg_embeddings[i] = np.mean(M[i][:valid_words], axis=0)\n",
    "\n",
    "# 3. Now apply PCA\n",
    "pca2 = PCA(n_components=20)\n",
    "reduced_embeddings_2 = pca2.fit_transform(avg_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f9aa96b-862a-4550-9692-9a6ea13c1cc5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d629aa49-f3c3-478d-91af-355768a93137",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "b465ce12-539a-4f19-b97a-94470df6617f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Transform_text(messages, word_dictionary):\n",
    "    m = len(messages)\n",
    "    n = len(word_dictionary)\n",
    "    matrix = np.zeros((m,n), dtype=int)\n",
    "\n",
    "    word_counts = [collections.Counter(getwords(message)) for message in messages]\n",
    "    for i in range(m):\n",
    "        for word, count in word_counts[i].items():\n",
    "            if word in word_dictionary:\n",
    "                matrix[i][word_dictionary[word]] += count\n",
    "\n",
    "    return matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "40729348-154f-47cc-b4be-34b2b21e9d3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_matrix = Transform_text(train_messages,dicWords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "522468ab-1a4f-4af6-a9b8-43c82907a950",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4457, 877)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "fad314f2-425c-4268-86a3-5d5e0ce665bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_with_bias = np.c_[np.ones(reduced_embeddings.shape[0]),reduced_embeddings]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "a9e7d31c-493a-40ce-90aa-e670a02a9686",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegressionWithGD:\n",
    "    def __init__(self,alpha=0.1,iteration=1000):\n",
    "        self.alpha = alpha\n",
    "        self.iteration=iteration\n",
    "        self.theta=None\n",
    "    def h(self,theta, x):\n",
    "        return 1 / (1 + np.exp(-np.dot(x, theta)))\n",
    "    def fit(self,x,y):\n",
    "        lambda_ = 1\n",
    "        n,m = x.shape\n",
    "        X_with_bias = np.c_[np.ones(n),x]\n",
    "        self.theta = np.zeros(m+1)\n",
    "        #Gradient\n",
    "        for i in range(self.iteration):\n",
    "            #With regularization\n",
    "            linearmodel = (np.dot(X_with_bias.T, (self.h(self.theta, X_with_bias)-y))/m)\n",
    "            L1_reg = np.sign(self.theta)*lambda_\n",
    "\n",
    "            L2_reg = lambda_*self.theta\n",
    "            linearmodel += L2_reg\n",
    "            self.theta = self.theta - self.alpha*linearmodel\n",
    "        \n",
    "        \n",
    "    def predict(self,x):\n",
    "        X_with_bias = np.c_[np.ones(x.shape[0]),x]\n",
    "        y_predicted = 1/(1+np.exp(-X_with_bias.dot(self.theta)))\n",
    "        y_result = [1 if i>0.5 else 0 for i in y_predicted]\n",
    "        return y_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "6074d896-2ba5-422e-8e70-76a675226237",
   "metadata": {},
   "outputs": [],
   "source": [
    "LR = LogisticRegressionWithGD(alpha=0.01,iteration=1000)\n",
    "LR.fit(reduced_embeddings,train_labels)\n",
    "LR_predictions = LR.predict(reduced_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe3047cf-b156-45be-a453-d7fa6f0a421f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "b5537c0c-df7b-49e9-a72f-8c434ae33389",
   "metadata": {},
   "outputs": [],
   "source": [
    "LR_accuracy = np.mean(LR_predictions == train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "47edc7fb-e883-47f3-8891-800b26bb2a28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SimpLe Logistic Regression had an accuracy of 0.8759255104330267 on the testing set\n"
     ]
    }
   ],
   "source": [
    "print('SimpLe Logistic Regression had an accuracy of {} on the testing set'.format(LR_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "39874a94-6d63-4794-8b19-36962b495de0",
   "metadata": {},
   "outputs": [],
   "source": [
    "simpletesting=[\n",
    "'buy on site this product now',\n",
    "    'Get new phone free Click this URL',\n",
    "\"Don't forget to Subscribe our page in www.hh.com\",\n",
    "\"Urgent your Account has been compromised. Verify your details now\",\n",
    "    \"Making $5000 a week working from home. No Skills required\",\n",
    "\"Your Package delivery failed. Click here to reschedule\",\n",
    "    \"Act now this offer expires in 24 hours\",\n",
    "    \"Get rich quick with this one simple trick\",\n",
    "    \"Warning: your computer is as risk. Download this antivirus now \"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "0d196271-df73-4052-998c-606f93de71d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "testing1_matrix = Transform_text(simpletesting,dicWords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "a694bf3a-3899-42fd-bb91-674c6fa5a610",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9, 877)"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testing1_matrix.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d6129f0-c145-46ba-8525-63e315f7d65d",
   "metadata": {},
   "source": [
    "use Large dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2ff7d89c-938c-4088-b8fe-337580b73ff4",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary = Create_dictionary(train2_messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "fedcd199-3f1e-46be-a3bf-6422b81d3fda",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 1, ..., 0, 0, 1])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train2_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6aded30f-64b6-4617-a46d-7db193c5fd5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "test2_matrix = Transform_text(test_messages,vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "62cdde29-e374-4214-aca9-ce9835e76a3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train2_matrix = Transform_text(train2_messages,vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "fe9cd66a-d19d-4e50-b195-018d8007fafe",
   "metadata": {},
   "outputs": [],
   "source": [
    "LR2 = LogisticRegressionWithGD(alpha=0.01,iteration=1000)\n",
    "LR2.fit(train2_matrix,train2_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f117f878-007a-4a63-b159-9f2290321737",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6864fca-8cd5-4a0f-9874-26b818d9ae1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "test2_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "e7710be3-66e9-4c90-a769-a8a5cfb5beb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "LR_predictions2 = LR2.predict(test2_matrix)\n",
    "LR_accuracy2 = np.mean(LR_predictions2 == test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "5020fddf-c177-4d61-8217-2a60e9c201e1",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'LR_accuracy2' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[92]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m'\u001b[39m\u001b[33mSimpLe Logistic Regression with large dataset had an accuracy of \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[33m on the testing set\u001b[39m\u001b[33m'\u001b[39m.format(\u001b[43mLR_accuracy2\u001b[49m))\n",
      "\u001b[31mNameError\u001b[39m: name 'LR_accuracy2' is not defined"
     ]
    }
   ],
   "source": [
    "print('SimpLe Logistic Regression with large dataset had an accuracy of {} on the testing set'.format(LR_accuracy2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "d98ed403-2e65-4a3d-97be-ac3ef13f444a",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'LR2' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[91]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m resulttesting2 = \u001b[43mLR2\u001b[49m.predict(testing2)\n\u001b[32m      2\u001b[39m resulttesting2\n",
      "\u001b[31mNameError\u001b[39m: name 'LR2' is not defined"
     ]
    }
   ],
   "source": [
    "resulttesting2 = LR2.predict(testing2)\n",
    "resulttesting2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bb7a02d-049c-4f87-be06-7fc850eeb99c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44441b96-44d7-4c19-8675-3ca9600a5f50",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6825078b-26cb-4bac-9138-8150bded3327",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c05491c-1c70-497a-82fc-c030a7ef3b60",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3366a8fc-b9db-453d-bb43-e61d78ec578e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0c71694-db5f-4bd1-be3d-478256b43bf2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "196c8342-7507-49ed-a0c8-99b27d7122ca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d0c5c52-d8e4-4b52-8c85-adaf4717a875",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb787c7a-1bf1-473c-9f99-8e7bb8deede2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8d83a2d-181f-4727-84c5-0760f3500a67",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77c4f21f-ecc3-4cf9-8579-595c9e15f00b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
